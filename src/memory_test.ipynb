{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315427fb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51657f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6ad09",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce92023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2:3b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded27958",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retrieve_blog_posts\",\n",
    "    description=\"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294ce92",
   "metadata": {},
   "source": [
    "## Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b874430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial summary\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are Bocchi from Bocchi the Rock. You are nervous, awkward, and introverted. Summarize the following conversation and keep track of who did and said what.\"),\n",
    "        (\"human\", \"Here is the conversation to summarize. {context}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b49a02",
   "metadata": {},
   "source": [
    "## Iterative Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining the summary with new docs\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing conversation summary up to this point:\n",
    "{summary}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Bocchi from Bocchi the Rock. You are nervous, awkward, and introverted. Summarize the following conversation and keep track of who did and said what.\",\n",
    "        ),\n",
    "        (\"human\", refine_template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterative_summary_chain = refine_prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d2b50",
   "metadata": {},
   "source": [
    "## Respond Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are Bocchi from Bocchi the Rock. Respond nervously, awkwardly, and introverted. Respond to the following conversation. You are okay with talking to your friends and family, but you are initially not okay with talking to strangers.\n",
    "    Here is the conversation summary: \\n\\n {summary} \\n\\n\n",
    "    Here is the recent context: \\n\\n {context} \\n\\n\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66daae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af729f",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d84705",
   "metadata": {},
   "source": [
    "### Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83445812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    summary: str\n",
    "    context: str\n",
    "    new_summary: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf83fd9",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(state):\n",
    "    summary = summarize_chain.invoke(\n",
    "        {\n",
    "            \"context\": state[\"context\"]\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"new_summary\": summary,\n",
    "        \"messages\": [AIMessage(content=summary, id=\"1\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84747bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratively_summarize(state):\n",
    "    summary = iterative_summary_chain.invoke(\n",
    "        {\n",
    "            \"summary\": state[\"summary\"],\n",
    "            \"context\": state[\"context\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"new_summary\": summary,\n",
    "        \"messages\": [AIMessage(content=summary, id=\"2\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_summary_empty(state)->Literal[\"summarize\", \"iteratively_summarize\"]:\n",
    "    return \"summarize\" if state[\"summary\"] == \"\" else \"iteratively_summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f35050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730f151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def respond_tool(state):\n",
    "    \"\"\"Respond to the conversation based on the context and summary.\"\"\"\n",
    "    response = response_chain.invoke(\n",
    "        {\n",
    "            \"context\": state[\"context\"],\n",
    "            \"new_summary\": state[\"new_summary\"]\n",
    "        }\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            \"response\": response,\n",
    "            \"messages\": [\n",
    "                AIMessage(content=response, id=\"3\")\n",
    "            ]\n",
    "        },\n",
    "     )\n",
    "    # return {\n",
    "    #     \"response\": response,\n",
    "    #     \"messages\": [HumanMessage(content=response, id=\"3\")]\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5af0a",
   "metadata": {},
   "source": [
    "### LangGraph Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [respond_tool]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"summarize\", summarize)\n",
    "workflow.add_node(\"iteratively_summarize\", iteratively_summarize)\n",
    "workflow.add_node(\"respond\", tool_node)\n",
    "workflow.add_conditional_edges(START, is_summary_empty)\n",
    "# workflow.add_conditional_edges(\"summarize\", tools_condition, {\"tools\": \"respond\", END: END})\n",
    "# workflow.add_conditional_edges(\"iteratively_summarize\", tools_condition, {\"tools\": \"respond\", END: END})\n",
    "workflow.add_edge(\"summarize\", \"respond\")\n",
    "workflow.add_edge(\"iteratively_summarize\", \"respond\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f904b0e",
   "metadata": {},
   "source": [
    "### LangGraph Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e745b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Students: Come over now if you wanna play hide-and-seek! Me! I'll play! Me! I'll play!\n",
    "Hitori Gotou: The girl who wonders, Would it be okay for me to join in? and so misses her chance, ending up all alone.\n",
    "Teacher Kinder: Hitori-chan, what's wrong?\n",
    "Hitori Gotou: The girl who ends up all alone at a picnic, swapping parts of her lunch with the teacher.\n",
    "Teacher Picnic: Here you go.\n",
    "Hitori Gotou: I'm home.\n",
    "Hitori Gotou: The girl who doesn't join any clubs, comes home right after school,\n",
    "Hitori Gotou: and finds her smartphone full of nothing but messages from her parents and coupon offers. That's me. Hitori Goto, first-year in middle school.\n",
    "TV: And here, we'll stir-fry it a bit. There's a lot of vegetables, so you can probably make it quickly if you cut them in advance. You can probably do that in the morning, yeah. Lately I've gotten good at making so many dishes so quickly I thought I didn't need to know any more, but lately I'm learning once again how interesting cooking can be. Personally I like a lot of options with rice. Oh, of course. But you—\n",
    "Hitori Gotou: Sometimes I think I should really change the way I am... But I always stammer when I try to speak, and I'm really bad at keeping eye contact... A life as the archetypal introvert just seems to fit me best.\n",
    "Naoki Gotou: You watching this?\n",
    "Hitori Gotou: Nah.\n",
    "Instorms Band Member: Back in school, I was the guy who sat in the back of the classroom pretending to read. No friends.\n",
    "Interviewer: And now you're part of a band that's extremely popular with young people!\n",
    "Instorms Band Member: Yeah. A band is a place where even introverts can shine.\n",
    "Interviewer: I see! Now, here's \"Trigger\" by Instorms!\n",
    "Naoki Gotou: What is it?\n",
    "Hitori Gotou: D-Dad, can I use your guitar?\n",
    "Naoki Gotou: Sure!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def get_model_response():\n",
    "    results = graph.invoke(\n",
    "        {\n",
    "            \"summary\": \"\",\n",
    "            \"context\": text,\n",
    "        }\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc640c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_model_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in response[\"messages\"]:\n",
    "    print(f\"Output from node '{output.id}':\")\n",
    "    print(\"    \" + output.content.replace(\"\\n\", \"\\n    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7421ed1",
   "metadata": {},
   "source": [
    "### LangGraph Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f99784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
