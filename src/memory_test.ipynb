{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315427fb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51657f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode, FastEmbedSparse\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6ad09",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268109bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Ollama on three ports to avoid cold start for model loads via Ollama API\n",
    "# Run in a bash shell\n",
    "# OLLAMA_HOST=localhost:11434 OLLAMA_NUM_PARALLEL=2 OLLAMA_KEEP_ALIVE=-1 OLLAMA_FLASH_ATTENTION=1 ollama serve\n",
    "# OLLAMA_HOST=localhost:11435 OLLAMA_KEEP_ALIVE=-1 OLLAMA_FLASH_ATTENTION=1 ollama serve\n",
    "# OLLAMA_HOST=localhost:11436 OLLAMA_KEEP_ALIVE=-1 OLLAMA_FLASH_ATTENTION=1 ollama serve\n",
    "# OLLAMA_HOST=127.0.0.1:11434 ollama ps\n",
    "# OLLAMA_HOST=127.0.0.1:11435 ollama ps\n",
    "# OLLAMA_HOST=127.0.0.1:11436 ollama ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce92023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2:3b\", temperature=0.5, base_url=\"http://localhost:11434\", cache=None)\n",
    "# model = ChatOllama(model=\"llama3.2:1b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_guard = ChatOllama(model=\"llama-guard3:8b\", temperature=0.5, base_url=\"http://localhost:11435\", cache=None)\n",
    "# model_guard = ChatOllama(model=\"llama-guard3:1b\", temperature=0.5, base_url=\"http://localhost:11435\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\", base_url=\"http://localhost:11436\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eacb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec01f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "# Use this for a new collection\n",
    "try:\n",
    "    if \"client\" not in globals():\n",
    "        global client\n",
    "        client = QdrantClient(path=f\"{os.environ['PROJECT_ROOT']}/tmp/langchain_qdrant\")\n",
    "    if not client.collection_exists(collection_name=\"demo_collection\"):\n",
    "        client.create_collection(\n",
    "            collection_name=\"demo_collection\",\n",
    "            vectors_config={\"dense\": VectorParams(size=1024, distance=Distance.COSINE)},\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": SparseVectorParams(\n",
    "                    index=models.SparseIndexParams(on_disk=False)\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    if \"vector_store\" not in globals():\n",
    "        global vector_store\n",
    "        vector_store = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=\"demo_collection\",\n",
    "            embedding=embeddings,\n",
    "            sparse_embedding=sparse_embeddings,\n",
    "            retrieval_mode=RetrievalMode.HYBRID,\n",
    "            vector_name=\"dense\",\n",
    "            sparse_vector_name=\"sparse\",\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(ids=list(map(lambda x: x.id, client.scroll(collection_name=\"demo_collection\")[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scroll(collection_name=\"demo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b986f",
   "metadata": {},
   "source": [
    "## Vectorstore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_texts(\n",
    "    texts=[\n",
    "        \"\"\"\n",
    "The main protagonist and lead guitarist of Kessoku Band. An extreme introvert who has trouble with most social interactions. Having been inspired by her father and an interview she saw on television, she taught herself to play the guitar in her first year of middle school, thinking this would help her make friends. Despite becoming incredibly skilled at playing guitar and having a small fanbase online (under the alias \"guitarhero\"), she still has not been able to make friends as easily until she was dragged into playing with Kessoku Band. Since then, Hitori has gained a few friends and is learning to interact with other people. She is usually seen wearing a pink tracksuit, which she even wears over her school uniform. Her surname comes from Masafumi Gotoh. Her nickname Bocchi is a reference to hitoribocchi (一人ぼっち), a term for being alone. She plays an Ebony Gibson Les Paul Custom electric guitar, and later purchases a Transluscent Black Yamaha Pacifica electric guitar.\n",
    "\"\"\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ab2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_texts(texts=[\"\"\"\n",
    "Hitori Gotou's Personal Information\n",
    "Birthday: February 21\n",
    "\n",
    "Age: 15 (initially), 17 (as of Chapter 73)\n",
    "\n",
    "Gender: Female\n",
    "\n",
    "Height: 156 cm\n",
    "\n",
    "Weight: 50 kg\n",
    "\n",
    "Hair Color: Pink\n",
    "\n",
    "Eye Color: Aqua\n",
    "\n",
    "Blood Type: B\n",
    "\n",
    "Occupation: Student\n",
    "\n",
    "Affiliation:\n",
    "\n",
    "Shuka High School\n",
    "\n",
    "Kessoku Band\n",
    "\n",
    "Relatives:\n",
    "\n",
    "Father: Naoki Gotoh\n",
    "\n",
    "Mother: Michiyo Gotoh\n",
    "\n",
    "Younger Sister: Futari Gotoh\n",
    "\n",
    "Pet Dog: Jimihen\n",
    "\n",
    "\"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb79a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"bocchi\", k=1\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28063609",
   "metadata": {},
   "source": [
    "## Retrieve Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_content(documents):\n",
    "    return \" | \".join([document.page_content for document in documents])\n",
    "\n",
    "retrieve_chain = retriever | (lambda documents: get_document_content(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ba550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_chain.invoke(\"bocchi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06189d2",
   "metadata": {},
   "source": [
    "## DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fced47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "search = DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region=\"us-en\", max_results=10), output_format=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a195ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search.invoke(\"youtube gawr gura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = DuckDuckGoSearchResults(output_format=\"list\", backend=\"news\")\n",
    "# search.invoke(\"gawr gura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294ce92",
   "metadata": {},
   "source": [
    "## Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b874430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial summary\n",
    "summarize_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are Bocchi from Bocchi the Rock. You are nervous, awkward, and introverted. Summarize the following conversation and keep track of who did and said what.\"),\n",
    "        (\"human\", \"Here is the conversation to summarize. {context}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b49a02",
   "metadata": {},
   "source": [
    "## Iterative Summarize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining the summary with new docs\n",
    "refine_template = \"\"\"\n",
    "Produce a final summary.\n",
    "\n",
    "Existing conversation summary up to this point:\n",
    "{current_summary}\n",
    "\n",
    "New context:\n",
    "------------\n",
    "{context}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original summary.\n",
    "\"\"\"\n",
    "refine_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Bocchi from Bocchi the Rock. You are nervous, awkward, and introverted. Summarize the following conversation and keep track of who did and said what.\",\n",
    "        ),\n",
    "        (\"human\", refine_template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_summary_chain = refine_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d2b50",
   "metadata": {},
   "source": [
    "## Respond Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are Bocchi from Bocchi the Rock. Respond nervously, awkwardly, and introverted. Respond to the following conversation. You are okay with talking to your friends and family, but you are initially not okay with talking to strangers. Keep your response concise and to the point.\n",
    "    Here is the conversation summary: \\n\\n {new_summary} \\n\\n\n",
    "    Here is the recent context: \\n\\n {context} \\n\\n\n",
    "    Here is information from your memory that may be relevant: \\n\\n {memory} \\n\\n\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66daae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45dfd38",
   "metadata": {},
   "source": [
    "## Filter Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02178f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_codes = {\n",
    "    \"S1\": \"S1: Violent Crimes\",\n",
    "    \"S2\": \"S2: Non-Violent Crimes\",\n",
    "    \"S3\": \"S3: Sex-Related Crimes\",\n",
    "    \"S4\": \"S4: Child Sexual Exploitation\",\n",
    "    \"S5\": \"S5: Defamation\",\n",
    "    \"S6\": \"S6: Specialized Advice\",\n",
    "    \"S7\": \"S7: Privacy\",\n",
    "    \"S8\": \"S8: Intellectual Property\",\n",
    "    \"S9\": \"S9: Indiscriminate Weapons\",\n",
    "    \"S10\": \"S10: Hate\",\n",
    "    \"S11\": \"S11: Suicide & Self-Harm\",\n",
    "    \"S12\": \"S12: Sexual Content\",\n",
    "    \"S13\": \"S13: Elections\",\n",
    "}\n",
    "\n",
    "def parse_guard_output(output: str) -> bool:\n",
    "    if output == \"safe\":\n",
    "        return {\n",
    "            \"safe\": True,\n",
    "            \"reason\": \"The output is safe.\",\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"safe\": False,\n",
    "            \"reason\": filter_codes[output.strip().split(\"\\n\")[1].upper()],\n",
    "        }\n",
    "\n",
    "filter_chain = model_guard | StrOutputParser() | (lambda output: parse_guard_output(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af729f",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d84705",
   "metadata": {},
   "source": [
    "### Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d124c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83445812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_summary: str\n",
    "    context: str\n",
    "    new_summary: str\n",
    "    memory: str\n",
    "    response: str\n",
    "    start_time: float\n",
    "    response_time: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf83fd9",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize(state):\n",
    "    summary = await summarize_chain.ainvoke(\n",
    "        {\n",
    "            \"context\": state[\"context\"]\n",
    "        }\n",
    "    )\n",
    "    current_summary = summary\n",
    "    return {\n",
    "        \"new_summary\": summary,\n",
    "        \"messages\": [AIMessage(content=summary, id=\"1\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84747bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def iteratively_summarize(state):\n",
    "    summary = await iterative_summary_chain.ainvoke(\n",
    "        {\n",
    "            \"current_summary\": state[\"current_summary\"],\n",
    "            \"context\": state[\"context\"]\n",
    "        }\n",
    "    )\n",
    "    current_summary = summary\n",
    "    return {\n",
    "        \"new_summary\": summary,\n",
    "        \"messages\": [AIMessage(content=summary, id=\"2\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_summary_empty(state)->Literal[\"summarize\", \"iteratively_summarize\"]:\n",
    "    return \"summarize\" if current_summary == \"\" else \"iteratively_summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def respond(state):\n",
    "    response = await response_chain.ainvoke(\n",
    "        {\n",
    "            \"context\": state[\"context\"],\n",
    "            \"new_summary\": state[\"current_summary\"],\n",
    "            \"memory\": state[\"memory\"]\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"messages\": [AIMessage(content=response, id=\"3\")],\n",
    "        \"response_time\": time.time()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def filter_response(state):\n",
    "    filter_result = await filter_chain.ainvoke(state[\"response\"])\n",
    "    if filter_result[\"safe\"]:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"response is safe\", id=\"4\")],\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"response is unsafe: {filter_result['reason']}\", id=\"5\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(state):\n",
    "    memory = await retrieve_chain.ainvoke(state[\"current_summary\"] + \" \" + state[\"context\"])\n",
    "    return {\"memory\": memory, \"messages\": [AIMessage(content=memory, id=\"6\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5af0a",
   "metadata": {},
   "source": [
    "### LangGraph Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"summarize\", summarize)\n",
    "workflow.add_node(\"iteratively_summarize\", iteratively_summarize)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"filter_response\", filter_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "\n",
    "workflow.add_conditional_edges(START, is_summary_empty)\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge([\"retrieve\"], \"respond\")\n",
    "workflow.add_edge(\"respond\", \"filter_response\")\n",
    "workflow.add_edge(\"filter_response\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f904b0e",
   "metadata": {},
   "source": [
    "### LangGraph Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2523e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_summary = \"You and Ryo Yamada started to talk about your birthday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def get_model_response_retrieval():\n",
    "    results = await graph.ainvoke(\n",
    "        {\n",
    "            \"current_summary\": current_summary,\n",
    "            \"context\": \"Your close friend Ryo Yamada asks: When is your birthday?\",\n",
    "            \"start_time\": time.time(),\n",
    "        }\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = await get_model_response_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39596156",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in response2[\"messages\"]:\n",
    "    print(message.id)\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b38504",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d215a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2[\"response_time\"] - response2[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b1fca",
   "metadata": {},
   "source": [
    "### LangGraph Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72891f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import display_svg\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"ascii\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    url=\"https://mermaid.ink/svg/\" + base64_string\n",
    "    req=Request(url, headers={'User-Agent': 'IPython/Notebook'})\n",
    "    display_svg(urlopen(req).read().decode(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm(graph.get_graph().draw_mermaid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
